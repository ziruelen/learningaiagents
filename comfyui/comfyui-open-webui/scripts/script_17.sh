# Limitar VRAM de Ollama
docker exec -it ollama ollama run llama3.1:8b --num-gpu-layers 20