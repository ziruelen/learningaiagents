#!/bin/bash
# Script de instalaci√≥n r√°pida para ComfyUI + Open WebUI + Ollama

set -e

echo "üöÄ Instalando stack ComfyUI + Open WebUI + Ollama..."

# Crear directorios necesarios
mkdir -p comfyui_models/Stable-diffusion
mkdir -p comfyui_models/VAE
mkdir -p comfyui_models/Lora
mkdir -p comfyui_output
mkdir -p workflows

# Verificar Docker
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker no est√° instalado. Instala Docker primero."
    exit 1
fi

# Verificar Docker Compose
if ! command -v docker-compose &> /dev/null && ! docker compose version &> /dev/null; then
    echo "‚ùå Docker Compose no est√° instalado. Instala Docker Compose primero."
    exit 1
fi

# Verificar NVIDIA GPU (opcional pero recomendado)
if command -v nvidia-smi &> /dev/null; then
    echo "‚úÖ GPU NVIDIA detectada:"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
else
    echo "‚ö†Ô∏è  GPU NVIDIA no detectada. ComfyUI funcionar√° con CPU (muy lento)."
fi

# Levantar servicios
echo "üì¶ Levantando contenedores..."
docker-compose up -d

# Esperar a que los servicios est√©n listos
echo "‚è≥ Esperando a que los servicios est√©n listos..."
sleep 10

# Verificar Ollama
echo "üîç Verificando Ollama..."
if docker exec ollama ollama list &> /dev/null; then
    echo "‚úÖ Ollama funcionando"
else
    echo "‚ö†Ô∏è  Ollama a√∫n no est√° listo. Espera unos segundos m√°s."
fi

# Verificar ComfyUI
echo "üîç Verificando ComfyUI..."
if curl -s http://localhost:8188/queue &> /dev/null; then
    echo "‚úÖ ComfyUI funcionando"
else
    echo "‚ö†Ô∏è  ComfyUI a√∫n no est√° listo. Espera unos segundos m√°s."
fi

# Verificar Open WebUI
echo "üîç Verificando Open WebUI..."
if curl -s http://localhost:3000 &> /dev/null; then
    echo "‚úÖ Open WebUI funcionando"
else
    echo "‚ö†Ô∏è  Open WebUI a√∫n no est√° listo. Espera unos segundos m√°s."
fi

echo ""
echo "‚úÖ Instalaci√≥n completada!"
echo ""
echo "üìã Servicios disponibles:"
echo "  - Ollama: http://localhost:11434"
echo "  - ComfyUI: http://localhost:8188"
echo "  - Open WebUI: http://localhost:3000"
echo ""
echo "üìñ Pr√≥ximos pasos:"
echo "  1. Descarga un modelo de Ollama: docker exec -it ollama ollama pull llama3.1:8b"
echo "  2. Descarga un modelo de Stable Diffusion a comfyui_models/Stable-diffusion/"
echo "  3. Accede a Open WebUI: http://localhost:3000"
echo "  4. Configura la integraci√≥n con ComfyUI (ver art√≠culo completo)"

