# LangChain vs LlamaIndex - Ejemplos RAG para Homelab

## üìã Descripci√≥n

Ejemplos de c√≥digo y configuraciones del art√≠culo **"LangChain vs LlamaIndex: Framework RAG para Homelab (Gu√≠a Comparativa 2025)"** publicado en [ElDiarioIA.es](https://www.eldiarioia.es).

Este repositorio contiene implementaciones funcionales de sistemas RAG usando ambos frameworks con Qdrant (vector database) y Ollama (LLM local).

## üìÅ Estructura

```
langchain-vs-llamaindex/
‚îú‚îÄ‚îÄ docker-compose.langchain.yml    # Stack completo LangChain
‚îú‚îÄ‚îÄ docker-compose.llamaindex.yml   # Stack completo LlamaIndex
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ langchain_rag_example.py   # Ejemplo funcional LangChain
‚îÇ   ‚îî‚îÄ‚îÄ llamaindex_rag_example.py  # Ejemplo funcional LlamaIndex
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ rag_config.yaml             # Configuraci√≥n compartida
‚îî‚îÄ‚îÄ README.md
```

## üöÄ Uso R√°pido

### Prerrequisitos

- Docker y Docker Compose instalados
- GPU NVIDIA (opcional, recomendado para Ollama)
- Al menos 8GB RAM libre

### Opci√≥n 1: LangChain

```bash
# 1. Clonar o descargar este repositorio
cd langchain-vs-llamaindex

# 2. Crear directorio de datos
mkdir -p data
echo "Tu contenido aqu√≠..." > data/documents.txt

# 3. Levantar servicios
docker-compose -f docker-compose.langchain.yml up -d

# 4. Esperar a que Ollama descargue modelos (primera vez)
docker logs -f ollama-langchain

# 5. Ejecutar ejemplo
docker exec -it langchain-rag python scripts/langchain_rag_example.py
```

### Opci√≥n 2: LlamaIndex

```bash
# 1. Clonar o descargar este repositorio
cd langchain-vs-llamaindex

# 2. Crear directorio de datos
mkdir -p data
echo "Tu contenido aqu√≠..." > data/documents.txt

# 3. Levantar servicios
docker-compose -f docker-compose.llamaindex.yml up -d

# 4. Esperar a que Ollama descargue modelos (primera vez)
docker logs -f ollama-llamaindex

# 5. Ejecutar ejemplo
docker exec -it llamaindex-rag python scripts/llamaindex_rag_example.py
```

## üîß Configuraci√≥n

Edita `configs/rag_config.yaml` para ajustar:

- **Modelos Ollama**: Cambia `ollama.model` y `ollama.embedding_model`
- **Qdrant**: Ajusta URL y puerto si es necesario
- **Chunking**: Modifica `documents.chunk_size` y `chunk_overlap`
- **Retrieval**: Ajusta `retrieval.top_k` para m√°s/menos chunks

## üìö Modelos Ollama Recomendados

```bash
# LLM principal
ollama pull llama3

# Embeddings
ollama pull nomic-embed-text
```

Alternativas:
- `mistral`, `neural-chat`, `codellama` (LLMs)
- `all-minilm` (embeddings m√°s peque√±os)

## üêõ Troubleshooting

### Error: "Connection refused" con Qdrant

```bash
# Verificar que Qdrant est√© corriendo
docker ps | grep qdrant

# Si no est√°, reiniciar
docker-compose -f docker-compose.langchain.yml restart qdrant
```

### Error: "Model not found" en Ollama

```bash
# Listar modelos disponibles
docker exec -it ollama-langchain ollama list

# Descargar modelo faltante
docker exec -it ollama-langchain ollama pull llama3
```

### Out of Memory

- Reduce `chunk_size` en `rag_config.yaml`
- Usa modelos m√°s peque√±os (ej: `mistral` en lugar de `llama3`)
- Aumenta swap o RAM disponible

## üìñ Art√≠culo Completo

Para entender las diferencias entre LangChain y LlamaIndex, cu√°ndo usar cada uno, y mejores pr√°cticas, lee el art√≠culo completo:

**[LangChain vs LlamaIndex: Framework RAG para Homelab (Gu√≠a Completa 2025)](https://www.eldiarioia.es)**

## üîó Enlaces Relacionados

- [Qdrant Documentation](https://qdrant.tech/documentation/)
- [Ollama Documentation](https://ollama.ai/docs)
- [LangChain Documentation](https://python.langchain.com/)
- [LlamaIndex Documentation](https://docs.llamaindex.ai/)

## üìù Licencia

Estos ejemplos son de c√≥digo abierto y est√°n disponibles para uso educativo y personal.

---

**¬øPreguntas o problemas?** Abre un issue en el repositorio o consulta el art√≠culo completo.

